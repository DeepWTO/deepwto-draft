### utils
fastext facebook 

### repos

https://github.com/zalandoresearch/flair
https://github.com/guillaumegenthial/sequence_tagging


### web
https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f


### Char Embedding

codes: http://colinmorris.github.io/blog/1b-words-char-embeddings

paper: https://arxiv.org/pdf/1509.01626.pdf

### Fine Tuning

##### CharEmbed

##### WordEmbed

##### Refs

[word-char-embed pypi module](https://pypi.org/project/keras-word-char-embd/)
    
[model code](https://github.com/CyberZHG/keras-word-char-embd/blob/master/demo/sentiment_analysis.py) 

##### Issues 
- [ ] If performs fine tuning, determine the eval metric 
- [ ] Research over joint training of Embedding Layer and Task
    

##### TL;DR
Attention is All you Need with iPynb  
http://nlp.seas.harvard.edu/2018/04/03/attention.html

#### Simple Facts
Attention model is aliased to Transformer 
Attention model is released in tensor2tensor model 
Transformer has become ubiquitous nowadays 